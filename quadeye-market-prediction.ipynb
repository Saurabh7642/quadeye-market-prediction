{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":128086,"databundleVersionId":15331979,"sourceType":"competition"}],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoostRegressor, Pool\n\nCFG = {\n    'iterations': 2000,\n    'learning_rate': 0.03,\n    'depth': 6,\n    'loss_function': 'RMSE',\n    'l2_leaf_reg': 5.0,        \n    'shrinkage_factor': 0.15, \n    'random_seed': 42\n}\n\ndef load_data():\n    print(\"Loading data...\")\n    train_path, test_path = \"\", \"\"\n    for dirname, _, filenames in os.walk('/kaggle/input'):\n        for filename in filenames:\n            if filename == 'train.csv': train_path = os.path.join(dirname, filename)\n            if filename == 'test.csv': test_path = os.path.join(dirname, filename)\n    \n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    submission_template = test[['Id']].copy()\n    \n    return train, test, submission_template\n\n# --- 2. RANK-GAUSS FEATURE ENGINEERING ---\ndef process_data(df):\n    # Sort for time consistency\n    df = df.sort_values(['date_id', 'time_id', 'symbol_id']).reset_index(drop=True)\n    \n    f_cols = [c for c in df.columns if c.startswith('f')]\n    \n    # A. Cross-Sectional Rank-Gauss\n    # Instead of raw values, we convert to percentiles, then to Normal Distribution (Gauss)\n    # This makes the features look like a Bell Curve, \n    for col in f_cols:\n        # Group by Time -> Rank (0..1)\n        df[f'{col}_rank'] = df.groupby(['date_id', 'time_id'])[col].transform(\n            lambda x: x.rank(pct=True, method='first')\n        )\n        # Shift slightly to avoid infinity in Gauss transform\n        df[f'{col}_rank'] = df[f'{col}_rank'] - 0.5\n        \n    # B. Volatility Scaling \n\n    df['market_vol'] = df.groupby(['date_id', 'time_id'])[f_cols[0]].transform('std')\n    \n    # Feature Selection: Use Ranks + Volatility Interaction\n    keep_cols = [c for c in df.columns if '_rank' in c] + ['market_vol']\n    \n    # Memory optimization\n    for c in keep_cols:\n        df[c] = df[c].astype(np.float32)\n        \n    return df, keep_cols\n\n# --- 3. EXECUTION ---\ntrain, test, submission_template = load_data()\n\nprint(\"Applying Rank-Gauss Transform...\")\ntrain, features = process_data(train)\ntest, _ = process_data(test)\n\n# Clipping target to avoid explosion\ny_min, y_max = train['y'].quantile(0.005), train['y'].quantile(0.995)\ntrain['y'] = train['y'].clip(y_min, y_max)\n\n# Time Split\ndates = train['date_id'].unique()\nsplit_date = dates[int(len(dates) * 0.9)] # Train on 90%\n\nX_train = train[train['date_id'] <= split_date][features]\ny_train = train[train['date_id'] <= split_date]['y']\nX_val = train[train['date_id'] > split_date][features]\ny_val = train[train['date_id'] > split_date]['y']\n\nprint(f\"Training CatBoost on {len(features)} Rank-Features...\")\n\n# --- 4. MODELING (CatBoost) ---\n\nmodel = CatBoostRegressor(\n    iterations=CFG['iterations'],\n    learning_rate=CFG['learning_rate'],\n    depth=CFG['depth'],\n    loss_function=CFG['loss_function'],\n    l2_leaf_reg=CFG['l2_leaf_reg'],\n    random_seed=CFG['random_seed'],\n    boosting_type='Ordered', \n    verbose=100,\n    allow_writing_files=False,\n    task_type=\"CPU\" \n)\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=(X_val, y_val),\n    early_stopping_rounds=50,\n    use_best_model=True\n)\n\n# 5. PREDICTION & POST-PROCESSING\nprint(\"Predicting...\")\npreds = model.predict(test[features])\n\n# A. ZERO-SUM CENTERING \n# The sum of predictions for a given time must be 0\ntest['raw_pred'] = preds\nmarket_means = test.groupby(['date_id', 'time_id'])['raw_pred'].transform('mean')\npreds = preds - market_means\n\n# B. GLOBAL SHRINKAGE \n\npreds = preds * CFG['shrinkage_factor']\n\n# Clip to safety bounds\npreds = np.clip(preds, y_min, y_max)\n\n\nsubmission_template['y'] = preds\nsubmission_template.to_csv('submission.csv', index=False)\nprint(\"SUCCESS. Submission Generated.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:00:32.597047Z","iopub.execute_input":"2026-01-24T18:00:32.597344Z","iopub.status.idle":"2026-01-24T18:09:40.647225Z","shell.execute_reply.started":"2026-01-24T18:00:32.597315Z","shell.execute_reply":"2026-01-24T18:09:40.646141Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nApplying Rank-Gauss Transform...\nTraining CatBoost on 27 Rank-Features...\n0:\tlearn: 0.0018618\ttest: 0.0017012\tbest: 0.0017012 (0)\ttotal: 863ms\tremaining: 28m 45s\n100:\tlearn: 0.0018570\ttest: 0.0016973\tbest: 0.0016973 (100)\ttotal: 1m 10s\tremaining: 22m 8s\n200:\tlearn: 0.0018563\ttest: 0.0016971\tbest: 0.0016971 (200)\ttotal: 2m 20s\tremaining: 20m 58s\n300:\tlearn: 0.0018559\ttest: 0.0016971\tbest: 0.0016971 (296)\ttotal: 3m 32s\tremaining: 19m 58s\n400:\tlearn: 0.0018555\ttest: 0.0016971\tbest: 0.0016971 (381)\ttotal: 4m 43s\tremaining: 18m 51s\n500:\tlearn: 0.0018552\ttest: 0.0016970\tbest: 0.0016970 (500)\ttotal: 5m 55s\tremaining: 17m 44s\nStopped by overfitting detector  (50 iterations wait)\n\nbestTest = 0.001697020168\nbestIteration = 500\n\nShrink model to first 501 iterations.\nPredicting...\nSUCCESS. Submission Generated.\n","output_type":"stream"}],"execution_count":1}]}